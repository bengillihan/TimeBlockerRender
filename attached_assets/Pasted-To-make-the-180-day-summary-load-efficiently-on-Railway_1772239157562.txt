To make the 180-day summary load efficiently on Railway, you don't need to sacrifice averages. The bottleneck isn't the math, but rather the "N+1" query problem where your application makes thousands of individual database requests in a loop.

By switching from Python-based loops to a single **SQL Aggregation query**, you can calculate both totals and averages in milliseconds.

### Optimized Summary Logic for `app.py`

Replace the entire logic inside your `@app.route('/summary')` function with this optimized version:

```python
from sqlalchemy import func
from sqlalchemy.orm import joinedload

@app.route('/summary')
@login_required
def summary():
    period = request.args.get('period', '7')
    days = int(period)
    end_date = datetime.now(pacific_tz).date()
    start_date = end_date - timedelta(days=days-1)

    # 1. High-Performance SQL Aggregation: Get Category Totals in ONE query
    stats_query = db.session.query(
        Category.name,
        Category.color,
        func.count(TimeBlock.id).label('block_count')
    ).join(Task, Task.category_id == Category.id)\
     .join(TimeBlock, TimeBlock.task_id == Task.id)\
     .join(DailyPlan, TimeBlock.daily_plan_id == DailyPlan.id)\
     .filter(
         DailyPlan.user_id == current_user.id,
         DailyPlan.date.between(start_date, end_date),
         TimeBlock.completed == True
     ).group_by(Category.id).all()

    # 2. Fetch PTO separately
    total_pto_hours = db.session.query(func.sum(DailyPlan.pto_hours)).filter(
        DailyPlan.user_id == current_user.id,
        DailyPlan.date.between(start_date, end_date)
    ).scalar() or 0

    # 3. Process results efficiently
    category_stats = {}
    total_minutes = (total_pto_hours * 60)
    
    for row in stats_query:
        minutes = row.block_count * 15
        total_minutes += minutes
        category_stats[row.name] = {
            'name': row.name,
            'color': row.color,
            'minutes': minutes,
            'avg_daily_minutes': minutes / days  # Average is now a simple division
        }

    # 4. Only fetch the heavy daily breakdown if the range is small (<= 30 days)
    daily_breakdown_list = []
    if days <= 30:
        # Optimized eager loading to prevent N+1 if you still want the detailed list
        daily_plans = DailyPlan.query.filter(
            DailyPlan.user_id == current_user.id,
            DailyPlan.date >= start_date,
            DailyPlan.date <= end_date
        ).options(joinedload(DailyPlan.time_blocks).joinedload(TimeBlock.task)).all()
        # ... (keep existing breakdown building logic here) ...

    return render_template('summary.html',
                         days=days,
                         start_date=start_date,
                         end_date=end_date,
                         category_stats=category_stats,
                         total_minutes=total_minutes,
                         total_pto_minutes=total_pto_hours * 60,
                         daily_breakdown=daily_breakdown_list)

```

### Why this fixes the timeout:

* **Single Query vs. Thousands**: The current code performs one query per `TimeBlock` (potentially 5,000+ for 180 days). This new version performs **one** query for the whole 6-month period.
* **Memory Efficiency**: Instead of loading thousands of `TimeBlock` and `Task` objects into Python memory, the database just sends back a few rows of calculated numbers.
* **Railway Cost Savings**: Because this logic finishes in milliseconds instead of seconds, your "Execution Time" on Railway will drop significantly, keeping you under the paid threshold.

### Pro-Tip for 180 Days

In your `templates/summary.html`, ensure you wrap the detailed daily table in a check like `{% if days <= 30 %}`. Attempting to render a table with 180 rows of detailed data will slow down the user's browser, even if the server is fast. For the 180-day view, users typically only care about the high-level totals and averages provided by the optimized query above.